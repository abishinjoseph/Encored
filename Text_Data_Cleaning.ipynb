{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPydQUBuhbwuLPGYydQhCUd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abishinjoseph/Encored/blob/main/Text_Data_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEjw2PnuyKy3",
        "outputId": "bfb5a387-eabd-4fae-8db8-913a385c7e57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0                @VirginAmerica What @dhepburn said.   \n",
            "1  @VirginAmerica plus you've added commercials t...   \n",
            "2  @VirginAmerica I didn't today... Must mean I n...   \n",
            "3  @VirginAmerica it's really aggressive to blast...   \n",
            "4  @VirginAmerica and it's a really big bad thing...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0                        virginamerica dhepburn said  \n",
            "1  virginamerica plus youve added commercials exp...  \n",
            "2  virginamerica didnt today must mean need take ...  \n",
            "3  virginamerica really aggressive blast obnoxiou...  \n",
            "4                 virginamerica really big bad thing  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = \"Tweets.csv\"  # Update the path to the location of your downloaded file\n",
        "tweets_df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Define text cleaning function\n",
        "def text_cleaning(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '',text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    clean_text = ' '.join(filtered_tokens)\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "# Apply text cleaning to 'text' column in the dataset\n",
        "tweets_df['cleaned_text'] = tweets_df['text'].apply(text_cleaning)\n",
        "\n",
        "# Display the cleaned dataset\n",
        "print(tweets_df[['text', 'cleaned_text']].head())\n"
      ]
    }
  ]
}